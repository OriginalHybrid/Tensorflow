# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lQ88TK9EUPyNUpAYbpQfJnSin09QXaje
"""

import numpy as np 
import pandas as pd 
import tensorflow as tf 

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)


dataset = pd.read_csv(f"/Users/ashish.ed.kumar/Downloads/code/IOT-temp.csv")
series = pd.Series(data=dataset['temp'].values, index=dataset['noted_date'])

min = np.min(series)
max = np.max(series)
series -= min
series /= max 
time = np.array(dataset['noted_date'])

split_time = 80000

x_train = series[:split_time]
x_valid = series[split_time:]

len(x_train), len(x_valid)

window_size = 25
batch_size = 64
shuffle_buffer_size = 1000

train_set = windowed_dataset(x_train.astype(np.float32), window_size=window_size,
                                batch_size=batch_size,
                                shuffle_buffer=shuffle_buffer_size)

validation_set = windowed_dataset(x_valid.astype(np.float32), window_size=window_size,
                                batch_size=batch_size,
                                shuffle_buffer=shuffle_buffer_size)

tf.keras.backend.clear_session()

model = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=[None, 1]),
  # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), input_shape=[None, 1]),
  # tf.keras.layers.Dense(64),
  tf.keras.layers.Dense(1),
])
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, epochs=5, validation_data=validation_set)

