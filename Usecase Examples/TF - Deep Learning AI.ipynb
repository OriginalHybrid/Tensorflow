{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93bb41b4",
   "metadata": {},
   "source": [
    "# The Hello World of Deep Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b571a3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803c0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2f666",
   "metadata": {},
   "source": [
    "## Define and Compile a Simple Neural Network\n",
    "\n",
    "Next we will create the simplest possible neural network. It has 1 layer, and that layer has 1 neuron, and the input shape to it is just 1 value.\n",
    "Steps:\n",
    "- Build\n",
    "- Compile\n",
    "- Fit\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4030ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4fe09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66e5acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  0.,  1.,  2.,  3.,  4.]), array([-3., -1.,  1.,  3.,  5.,  7.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1319d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 50.5310\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 40.1515\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 31.9772\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.5381\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 20.4643\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.4649\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.3108\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.8220\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.8568\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3036\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0747\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.1011\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.3286\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7143\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2246\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8332\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5191\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2661\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0612\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8943\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7573\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6441\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5497\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4701\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4023\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3440\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2931\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2483\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2083\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1721\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1391\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1087\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0804\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0539\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0288\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0050\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9823\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9604\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9394\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9191\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8994\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8803\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8617\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8436\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8260\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8088\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7919\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7755\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7595\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7438\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7284\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7134\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6987\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6843\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6702\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6565\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6430\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6297\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6168\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6041\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5917\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5795\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5676\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5560\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5445\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5334\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5224\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5117\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5012\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4909\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4808\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4709\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4612\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4518\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4425\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4334\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4245\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4158\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4072\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3989\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3907\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3826\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3748\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3671\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3595\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3522\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3449\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3378\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3309\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3241\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3174\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3109\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3045\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2983\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2922\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2862\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2803\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2745\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2689\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2634\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2579\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2527\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2475\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2424\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2374\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2325\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2277\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2231\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2185\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2140\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2096\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2053\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2011\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1970\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1929\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1889\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1851\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1813\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1775\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1739\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1703\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1668\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1634\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1600\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1568\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1535\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1504\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1473\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1443\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1413\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1384\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1356\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1328\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1300\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1274\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1248\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1222\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1197\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1172\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1148\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1125\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1079\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1014\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0993\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0973\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0953\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0933\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0914\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0877\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0859\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0841\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0824\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0807\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0790\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0774\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0758\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0743\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0727\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0712\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0698\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0683\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0669\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0656\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0642\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0629\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0616\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0603\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0591\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0579\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0567\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0555\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0544\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0533\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0522\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0511\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0501\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0490\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0480\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0470\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0461\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0451\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0442\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0433\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0424\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0415\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0398\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0390\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0382\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0374\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0367\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0359\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0352\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0345\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0337\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0331\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0324\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0317\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0311\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0304\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0298\n",
      "Epoch 206/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0292\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0286\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0274\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0269\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0263\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0258\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0252\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0247\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0242\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0237\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0232\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0227\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0223\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0218\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0214\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0209\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0205\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0189\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0177\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0163\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0153\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0144\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0135\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0120\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0115\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0108\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0097\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0095\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0089\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0086\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0070\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0065\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0063\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0051\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0043\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 308/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0030\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9061e-04\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7026e-04\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.5033e-04\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.3081e-04\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1169e-04\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9296e-04\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7462e-04\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5666e-04\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.3906e-04\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2182e-04\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.0494e-04\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8841e-04\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7222e-04\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5635e-04\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4082e-04\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2560e-04\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1070e-04\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9610e-04\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8180e-04\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6780e-04\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5408e-04\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4064e-04\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2749e-04\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1460e-04\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0197e-04\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8961e-04\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.7750e-04\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6563e-04\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5402e-04\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4264e-04\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.3149e-04\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2058e-04\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0988e-04\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9941e-04\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8915e-04\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7910e-04\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6926e-04\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5962e-04\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5018e-04\n",
      "Epoch 408/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4093e-04\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3188e-04\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2301e-04\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1432e-04\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0581e-04\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9747e-04\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8931e-04\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8131e-04\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7348e-04\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6581e-04\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5829e-04\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5093e-04\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4373e-04\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3667e-04\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2975e-04\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2298e-04\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1634e-04\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0985e-04\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0348e-04\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9724e-04\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9114e-04\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8516e-04\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7930e-04\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7356e-04\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.6794e-04\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6244e-04\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5705e-04\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5177e-04\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4660e-04\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4154e-04\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3657e-04\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3171e-04\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2696e-04\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2229e-04\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1773e-04\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1326e-04\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0888e-04\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0458e-04\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0038e-04\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9627e-04\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9224e-04\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8829e-04\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8442e-04\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8063e-04\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7692e-04\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7329e-04\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6973e-04\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6624e-04\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6283e-04\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5948e-04\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5621e-04\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5300e-04\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4986e-04\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4678e-04\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4376e-04\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4081e-04\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3792e-04\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3508e-04\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3231e-04\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2959e-04\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2693e-04\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2432e-04\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2177e-04\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1926e-04\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1682e-04\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1442e-04\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1206e-04\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0976e-04\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0751e-04\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0530e-04\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0314e-04\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0102e-04\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8944e-05\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6911e-05\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.4920e-05\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2972e-05\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1061e-05\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9190e-05\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7359e-05\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5565e-05\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.3808e-05\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2086e-05\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.0400e-05\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8749e-05\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7131e-05\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5546e-05\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3996e-05\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2475e-05\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0986e-05\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9528e-05\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8101e-05\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6701e-05\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5333e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a99461b040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, ys, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5754fdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.97642]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([10.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1a6ba",
   "metadata": {},
   "source": [
    "## Simple Housing Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec237a5c",
   "metadata": {},
   "source": [
    "Build a neural network that predicts the price of a house according to a simple formula.\n",
    "\n",
    "So, imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc.\n",
    "\n",
    "How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc.\n",
    "\n",
    "Hint: Your network might work better if you scale the house price down. You don't have to give the answer 400...it might be better to create something that predicts the number 4, and then your answer is in the 'hundreds of thousands' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44745f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 1.7756\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8402\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4072\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2066\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1136\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0705\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0504\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0409\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0364\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0342\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0331\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0324\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0320\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0317\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0314\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0312\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0309\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0307\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0305\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0302\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0300\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0298\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0296\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0294\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0292\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0289\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0287\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0285\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0283\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0281\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0277\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0275\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0273\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0271\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0269\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0267\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0265\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0263\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0261\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0259\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0257\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0256\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0254\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0252\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0250\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0248\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0246\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0245\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0243\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0241\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0239\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0238\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0236\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0234\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0232\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0231\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0229\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0227\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0226\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0224\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0222\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0221\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0219\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0218\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0216\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0214\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0213\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0208\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0207\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0204\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0199\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0195\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0191\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0183\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0181\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0180\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0179\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0176\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0173\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0171\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0170\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0168\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 103/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0165\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0164\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0162\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0158\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0150\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0148\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0144\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0135\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0133\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0127\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0125\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0120\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0118\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0117\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0116\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0114\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0110\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0109\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0109\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0108\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0107\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0106\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0106\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0105\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0104\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0103\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0102\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0101\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0099\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0098\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0092\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0090\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0090\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0089\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0089\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0087\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0087\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0086\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0083\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 204/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0079\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0077\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0076\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0076\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0073\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0070\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0069\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0068\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0067\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0065\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0062\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0062\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0051\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0047\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0043\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0038\n",
      "Epoch 305/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0038\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0037\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0037\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0031\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0020\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 406/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0017\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0016\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0015\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9435e-04\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.8711e-04\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7992e-04\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7278e-04\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6569e-04\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.5866e-04\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.5167e-04\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.4474e-04\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3785e-04\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3102e-04\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.2424e-04\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1750e-04\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1082e-04\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.0419e-04\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9760e-04\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9106e-04\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.8457e-04\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7812e-04\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7173e-04\n",
      "Epoch 506/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6537e-04\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5907e-04\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5281e-04\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4660e-04\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4043e-04\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3431e-04\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2823e-04\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2219e-04\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1620e-04\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1026e-04\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0435e-04\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.9849e-04\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9268e-04\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8690e-04\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8117e-04\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7548e-04\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6983e-04\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6422e-04\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.5865e-04\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5312e-04\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4764e-04\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.4219e-04\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3678e-04\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3142e-04\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.2609e-04\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2080e-04\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1554e-04\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1033e-04\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0516e-04\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0002e-04\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9492e-04\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8986e-04\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.8483e-04\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7984e-04\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.7489e-04\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.6997e-04\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.6509e-04\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6024e-04\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5543e-04\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.5066e-04\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4592e-04\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4121e-04\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3654e-04\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3190e-04\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2730e-04\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2273e-04\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1819e-04\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1369e-04\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0922e-04\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0478e-04\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0037e-04\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.9600e-04\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9166e-04\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8734e-04\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8307e-04\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7882e-04\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.7460e-04\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7041e-04\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6626e-04\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6213e-04\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5804e-04\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5397e-04\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4994e-04\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4593e-04\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4195e-04\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.3800e-04\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3408e-04\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.3019e-04\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2633e-04\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2250e-04\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.1869e-04\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1491e-04\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1116e-04\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0743e-04\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0374e-04\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0007e-04\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9643e-04\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9281e-04\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8922e-04\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8566e-04\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8212e-04\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.7860e-04\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7512e-04\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.7166e-04\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6822e-04\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6481e-04\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6142e-04\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5806e-04\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5472e-04\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5141e-04\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4812e-04\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4486e-04\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4162e-04\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3840e-04\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3521e-04\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3203e-04\n",
      "Epoch 602/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2888e-04\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2576e-04\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2266e-04\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1958e-04\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1652e-04\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1349e-04\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1048e-04\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0749e-04\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0452e-04\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0157e-04\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9864e-04\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9574e-04\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9286e-04\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9000e-04\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8715e-04\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8433e-04\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8153e-04\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7875e-04\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7599e-04\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7325e-04\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7053e-04\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6783e-04\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6515e-04\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6249e-04\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5985e-04\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5723e-04\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5463e-04\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5205e-04\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4948e-04\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4693e-04\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4441e-04\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4190e-04\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3941e-04\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3693e-04\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3448e-04\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3204e-04\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2962e-04\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2722e-04\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2484e-04\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2247e-04\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2012e-04\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1779e-04\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1547e-04\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1318e-04\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1089e-04\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0863e-04\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0638e-04\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0415e-04\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0193e-04\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9973e-04\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9755e-04\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9538e-04\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9323e-04\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9109e-04\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8897e-04\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8687e-04\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8478e-04\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8270e-04\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8064e-04\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7860e-04\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7657e-04\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7455e-04\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7255e-04\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7057e-04\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6860e-04\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6664e-04\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6470e-04\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6277e-04\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6085e-04\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5895e-04\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5707e-04\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5519e-04\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5333e-04\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5149e-04\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4966e-04\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4784e-04\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4603e-04\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4424e-04\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4246e-04\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4069e-04\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3894e-04\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3720e-04\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3547e-04\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3375e-04\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3205e-04\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3036e-04\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2868e-04\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2702e-04\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2536e-04\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2372e-04\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2209e-04\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2047e-04\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1887e-04\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1727e-04\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1569e-04\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1412e-04\n",
      "Epoch 698/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1256e-04\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1101e-04\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0947e-04\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0794e-04\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0643e-04\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0493e-04\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0343e-04\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0195e-04\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0048e-04\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9902e-04\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9757e-04\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9613e-04\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9470e-04\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9328e-04\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9187e-04\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9048e-04\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8909e-04\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8771e-04\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8634e-04\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8498e-04\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8364e-04\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8230e-04\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8097e-04\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7965e-04\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7834e-04\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7704e-04\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7576e-04\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7447e-04\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7320e-04\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7194e-04\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7069e-04\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6944e-04\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6821e-04\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6699e-04\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6577e-04\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6456e-04\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6336e-04\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6217e-04\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6099e-04\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5982e-04\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5865e-04\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5750e-04\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5635e-04\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5521e-04\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5408e-04\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5296e-04\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5184e-04\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5074e-04\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4964e-04\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4855e-04\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4746e-04\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4639e-04\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4532e-04\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4427e-04\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4321e-04\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4217e-04\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4114e-04\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4011e-04\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3909e-04\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3807e-04\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3707e-04\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3607e-04\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3508e-04\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3409e-04\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3312e-04\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3215e-04\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3118e-04\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3023e-04\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2928e-04\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2834e-04\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2740e-04\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2647e-04\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2555e-04\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2464e-04\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2373e-04\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2283e-04\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2193e-04\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.2105e-04\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2016e-04\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1929e-04\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1842e-04\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1756e-04\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1670e-04\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1585e-04\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1501e-04\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1417e-04\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1334e-04\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1251e-04\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1169e-04\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1088e-04\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1007e-04\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0927e-04\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0847e-04\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0768e-04\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0690e-04\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0612e-04\n",
      "Epoch 794/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0534e-04\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0458e-04\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0381e-04\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0306e-04\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0231e-04\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0156e-04\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0082e-04\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0009e-04\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.9359e-05\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8634e-05\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7916e-05\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7202e-05\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6495e-05\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.5791e-05\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.5094e-05\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4401e-05\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3713e-05\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3031e-05\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2353e-05\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1680e-05\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1012e-05\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0349e-05\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9691e-05\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9037e-05\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.8388e-05\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7744e-05\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7105e-05\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6471e-05\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5841e-05\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5215e-05\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.4594e-05\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3978e-05\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.3366e-05\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2759e-05\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.2156e-05\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1558e-05\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.0963e-05\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.0373e-05\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9788e-05\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.9206e-05\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8630e-05\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.8057e-05\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7488e-05\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6924e-05\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.6364e-05\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5808e-05\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5255e-05\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4707e-05\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4163e-05\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3622e-05\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3086e-05\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2553e-05\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2024e-05\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1499e-05\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0978e-05\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0461e-05\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9948e-05\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.9438e-05\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8933e-05\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8431e-05\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7932e-05\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7437e-05\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6945e-05\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.6458e-05\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5974e-05\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5493e-05\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5016e-05\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4542e-05\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.4072e-05\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3605e-05\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3142e-05\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.2682e-05\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2225e-05\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1772e-05\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1322e-05\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0875e-05\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.0431e-05\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9991e-05\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9554e-05\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.9121e-05\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8690e-05\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.8261e-05\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.7837e-05\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.7416e-05\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6998e-05\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.6582e-05\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.6170e-05\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5761e-05\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5355e-05\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4951e-05\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4552e-05\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4154e-05\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3759e-05\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3368e-05\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2979e-05\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2593e-05\n",
      "Epoch 890/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2210e-05\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1830e-05\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1451e-05\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1078e-05\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.0705e-05\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0336e-05\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9969e-05\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9605e-05\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9243e-05\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8885e-05\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8529e-05\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8175e-05\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.7824e-05\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7475e-05\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7129e-05\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6786e-05\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6445e-05\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6106e-05\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5771e-05\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5437e-05\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5106e-05\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4777e-05\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4452e-05\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4128e-05\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.3806e-05\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3487e-05\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3170e-05\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2855e-05\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2543e-05\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2233e-05\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1926e-05\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1620e-05\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1317e-05\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1016e-05\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0717e-05\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0420e-05\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0126e-05\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9834e-05\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.9543e-05\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9255e-05\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8969e-05\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8685e-05\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8404e-05\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8123e-05\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7846e-05\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7570e-05\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7296e-05\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7025e-05\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6755e-05\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6487e-05\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6221e-05\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5958e-05\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5696e-05\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5435e-05\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.5177e-05\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4920e-05\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4666e-05\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4414e-05\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.4163e-05\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3914e-05\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3667e-05\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.3422e-05\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3178e-05\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2936e-05\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2697e-05\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2458e-05\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2222e-05\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1987e-05\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1754e-05\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.1523e-05\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1293e-05\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1065e-05\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0839e-05\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0614e-05\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.0391e-05\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0169e-05\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9949e-05\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9731e-05\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9515e-05\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.9299e-05\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9086e-05\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8874e-05\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8664e-05\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8455e-05\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8248e-05\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8042e-05\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7838e-05\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7635e-05\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7434e-05\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7234e-05\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7035e-05\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6838e-05\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6643e-05\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6449e-05\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6256e-05\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.6065e-05\n",
      "Epoch 986/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5874e-05\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5686e-05\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5499e-05\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5313e-05\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5129e-05\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4945e-05\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4764e-05\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4583e-05\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4404e-05\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4227e-05\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4050e-05\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3875e-05\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3701e-05\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3528e-05\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3357e-05\n",
      "[[4.006971]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "xs = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], dtype=float)\n",
    "ys = np.array([1.0, 1.5, 2.0, 2.5, 3.0, 3.5], dtype=float)\n",
    "model.fit(xs, ys, epochs=1000)\n",
    "print(model.predict([7.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478151d",
   "metadata": {},
   "source": [
    "## A Computer Vision Example - using simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d75027",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c2312",
   "metadata": {},
   "source": [
    "Calling load_data on this object will give you two sets of two lists, these will be the training and testing values for the graphics that contain the clothing items and their labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f167aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d3dbcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(training_images[0])\n",
    "print(training_labels[0])\n",
    "print(training_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb553b3",
   "metadata": {},
   "source": [
    "You'll notice that all of the values in the number are between 0 and 255. If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1, a process called 'normalizing'...and fortunately in Python it's easy to normalize a list like this without looping. You do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fede2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41acf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f062b4",
   "metadata": {},
   "source": [
    "**Sequential**: That defines a SEQUENCE of layers in the neural network\n",
    "\n",
    "**Flatten**: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
    "\n",
    "**Dense**: Adds a layer of neurons\n",
    "\n",
    "Each layer of neurons need an activation function to tell them what to do. There's lots of options, but just use these for now.\n",
    "\n",
    "**Relu** effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.\n",
    "\n",
    "**Softmax** takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!\n",
    "\n",
    "The next thing to do, now the model is defined, is to actually build it. You do this by compiling it with an optimizer and loss function as before -- and then you train it by calling model.fit asking it to fit your training data to your training labels -- i.e. have it figure out the relationship between the training data and its actual labels, so in future if you have data that looks like the training data, then it can make a prediction for what that data would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2710702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2827 - accuracy: 0.8973\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2688 - accuracy: 0.9033\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2531 - accuracy: 0.9041\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2408 - accuracy: 0.9110\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2378 - accuracy: 0.9097\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2238 - accuracy: 0.9176\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2213 - accuracy: 0.9168\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2134 - accuracy: 0.9210\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2055 - accuracy: 0.9232\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1997 - accuracy: 0.9253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab156b7940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "380393e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3517 - accuracy: 0.8749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3517008125782013, 0.8748999834060669]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)  ## For 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42ac437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33957913517951965, 0.8917999863624573]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10_epochs = model.evaluate(test_images, test_labels) # For 10 epochs\n",
    "model_10_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41e9245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.0102964e-08 3.2853443e-08 1.6935898e-09 ... 1.4635180e-02 4.2125353e-10 9.8526430e-01]\n",
      " [9.4179693e-04 2.4070812e-13 9.9673575e-01 ... 3.3279673e-15 9.6271123e-11 2.9702640e-12]\n",
      " [1.3067791e-10 1.0000000e+00 2.2241410e-13 ... 4.7111454e-29 5.6822221e-16 2.6661673e-27]\n",
      " ...\n",
      " [4.5322467e-06 4.2845192e-15 4.4446546e-07 ... 1.2834648e-09 9.9998748e-01 1.8366447e-21]\n",
      " [1.5125394e-08 9.9999857e-01 2.1124966e-14 ... 6.4173280e-26 3.3719152e-12 3.4813137e-20]\n",
      " [6.7385503e-07 8.0483875e-10 2.7746438e-07 ... 1.1108990e-05 2.5920835e-06 1.8358829e-08]]\n",
      "[3.01029637e-08 3.28534426e-08 1.69358982e-09 3.12834658e-07 7.93324226e-11 1.00082274e-04 1.21472397e-07 1.46351801e-02 4.21253532e-10 9.85264301e-01]\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "print(classifications)\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee6e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcda97dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9852643, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(classifications[0]), list(classifications[0]).index(max(classifications[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e605e19",
   "metadata": {},
   "source": [
    "It shows that model is **98.52643 %** sure that it is label **9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f604ae",
   "metadata": {},
   "source": [
    "## Increasing Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c68fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3152 - accuracy: 0.9047\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0791 - accuracy: 0.9762\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0475 - accuracy: 0.9850\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0338 - accuracy: 0.9895\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0217 - accuracy: 0.9928\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0701 - accuracy: 0.9806\n",
      "[3.5374648e-11 4.3549075e-08 2.1223356e-08 3.3635834e-07 1.2433110e-13 1.8366213e-10 1.0406238e-13 9.9999964e-01 1.0215457e-10 1.1501796e-09]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model_dense_1024 = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model_dense_1024.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_dense_1024.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model_dense_1024.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model_dense_1024.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23a22601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07005607336759567, 0.9805999994277954]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dense_1024_5epochs = model_dense_1024.evaluate(test_images, test_labels)\n",
    "model_dense_1024_5epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99a2580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### training using 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9131c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3084 - accuracy: 0.9084\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0743 - accuracy: 0.9764\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0460 - accuracy: 0.9856\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0313 - accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0228 - accuracy: 0.9922\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0154 - accuracy: 0.9947\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0135 - accuracy: 0.9953\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0105 - accuracy: 0.9965\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0126 - accuracy: 0.9956\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0785 - accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07845915853977203, 0.983299970626831]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_dense_1024_10epochs = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model_dense_1024_10epochs.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_dense_1024_10epochs.fit(training_images, training_labels, epochs=10)\n",
    "\n",
    "model_dense_1024_10epochs.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5a7c5",
   "metadata": {},
   "source": [
    "### What if you remove flatten layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "705b6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:755 train_step\n        loss = self.compiled_loss(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1568 sparse_categorical_crossentropy\n        return K.sparse_categorical_crossentropy(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4940 sparse_categorical_crossentropy\n        res = nn.sparse_softmax_cross_entropy_with_logits_v2(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4240 sparse_softmax_cross_entropy_with_logits_v2\n        return sparse_softmax_cross_entropy_with_logits(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4153 sparse_softmax_cross_entropy_with_logits\n        raise ValueError(\"Shape mismatch: The shape of labels (received %s) \"\n\n    ValueError: Shape mismatch: The shape of labels (received (32, 1)) should equal the shape of logits except for the last dimension (received (32, 28, 10)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-359ee4deab62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m               loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel_dense_1024_nf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel_dense_1024_nf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:755 train_step\n        loss = self.compiled_loss(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1568 sparse_categorical_crossentropy\n        return K.sparse_categorical_crossentropy(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4940 sparse_categorical_crossentropy\n        res = nn.sparse_softmax_cross_entropy_with_logits_v2(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4240 sparse_softmax_cross_entropy_with_logits_v2\n        return sparse_softmax_cross_entropy_with_logits(\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    E:\\InstallationDir\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4153 sparse_softmax_cross_entropy_with_logits\n        raise ValueError(\"Shape mismatch: The shape of labels (received %s) \"\n\n    ValueError: Shape mismatch: The shape of labels (received (32, 1)) should equal the shape of logits except for the last dimension (received (32, 28, 10)).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dense_1024_nf = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model_dense_1024_nf.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_dense_1024_nf.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model_dense_1024_nf.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505a6dc",
   "metadata": {},
   "source": [
    "### Adding Extra layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1b78e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3135 - accuracy: 0.9050\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0857 - accuracy: 0.9726\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0554 - accuracy: 0.9823\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0453 - accuracy: 0.9863\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0321 - accuracy: 0.9891\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0865 - accuracy: 0.9781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08651339262723923, 0.9781000018119812]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_dense_1024_extralayer = tf.keras.models.Sequential([ tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model_dense_1024_extralayer.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_dense_1024_extralayer.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model_dense_1024_extralayer.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75815e",
   "metadata": {},
   "source": [
    "### more layers and more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcf15cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3290 - accuracy: 0.9020\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0840 - accuracy: 0.9741\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0597 - accuracy: 0.9816: 0s - los\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0410 - accuracy: 0.9874\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0317 - accuracy: 0.9901\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0286 - accuracy: 0.9908\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0251 - accuracy: 0.9923\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0204 - accuracy: 0.9934\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0169 - accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0182 - accuracy: 0.9942\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10861458629369736, 0.9799000024795532]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_dense_1024_extralayer_10epochs = tf.keras.models.Sequential([ tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model_dense_1024_extralayer_10epochs.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_dense_1024_extralayer_10epochs.fit(training_images, training_labels, epochs=10)\n",
    "\n",
    "model_dense_1024_extralayer_10epochs.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd435e8",
   "metadata": {},
   "source": [
    "### Using Callback to stop training at best value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24f20a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.95):\n",
    "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fb34343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3235 - accuracy: 0.9023\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0840 - accuracy: 0.9728\n",
      "\n",
      "Reached 95% accuracy so cancelling training!\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.1023 - accuracy: 0.9685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10225726664066315, 0.968500018119812]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_dense_1024_extralayer_10epochs = tf.keras.models.Sequential([ tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "\n",
    "model_dense_1024_extralayer_10epochs.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_dense_1024_extralayer_10epochs.fit(training_images, training_labels, epochs=20, callbacks=[callbacks])\n",
    "\n",
    "model_dense_1024_extralayer_10epochs.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2e47f",
   "metadata": {},
   "source": [
    "## Handwriting Recogntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59cd392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3368 - accuracy: 0.9014\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0814 - accuracy: 0.9756\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0502 - accuracy: 0.9843\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0344 - accuracy: 0.9892\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0246 - accuracy: 0.9920\n",
      "\n",
      "Reached 99% accuracy so cancelling training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab30a8b5e0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.99):\n",
    "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "callbacks = myCallback()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "016dc720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0891 - accuracy: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08905239403247833, 0.973800003528595]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512156e",
   "metadata": {},
   "source": [
    "## CNN Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5a393",
   "metadata": {},
   "source": [
    "### Improving Computer Vision Accuracy using Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d78754ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.6320 - accuracy: 0.7843\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3863 - accuracy: 0.8608\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3383 - accuracy: 0.8769\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3067 - accuracy: 0.8889\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2887 - accuracy: 0.8945\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3410 - accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7745cac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34095537662506104, 0.8769999742507935]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45d2ec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 11s 3ms/step - loss: 0.6129 - accuracy: 0.7784\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3112 - accuracy: 0.8860\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2598 - accuracy: 0.9047\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2231 - accuracy: 0.9184\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1919 - accuracy: 0.9288\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3410 - accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model_convd = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_convd.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_convd.summary()\n",
    "model_convd.fit(training_images, training_labels, epochs=5)\n",
    "model_convd_test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c5a1642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34095537662506104, 0.8769999742507935]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_convd_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2b14a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 862,410\n",
      "Trainable params: 862,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5742 - accuracy: 0.7906\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2889 - accuracy: 0.8928\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2291 - accuracy: 0.9154\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1962 - accuracy: 0.9253\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1652 - accuracy: 0.9369\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model_convd_512 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_convd_512.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_convd_512.summary()\n",
    "model_convd_512.fit(training_images, training_labels, epochs=5)\n",
    "model_convd_512_test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3e94d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34095537662506104, 0.8769999742507935]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_convd_512_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ff4185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 924,234\n",
      "Trainable params: 924,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5981 - accuracy: 0.7774\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2871 - accuracy: 0.8929\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2315 - accuracy: 0.9121\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1952 - accuracy: 0.9259\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1674 - accuracy: 0.9362\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1430 - accuracy: 0.9453\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1184 - accuracy: 0.9554\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1020 - accuracy: 0.9615\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0855 - accuracy: 0.9682\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0759 - accuracy: 0.9721\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model_convd_512_2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_convd_512_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_convd_512_2.summary()\n",
    "model_convd_512_2.fit(training_images, training_labels, epochs=10)\n",
    "model_convd_512_2_test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8946527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34095537662506104, 0.8769999742507935]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_convd_512_2_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "952853ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2973 - accuracy: 0.9098\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0493 - accuracy: 0.9860\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0288 - accuracy: 0.9915\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0194 - accuracy: 0.9937\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0121 - accuracy: 0.9959\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0093 - accuracy: 0.9968\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0062 - accuracy: 0.9980\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0047 - accuracy: 0.9985\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0040 - accuracy: 0.9988\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0034 - accuracy: 0.9988\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0643 - accuracy: 0.9857\n",
      "0.06426792591810226 0.9857000112533569\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model_1_convd_1_dense = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_1_convd_1_dense.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_1_convd_1_dense.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model_1_convd_1_dense.evaluate(test_images, test_labels)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0946811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06426792591810226, 0.9857000112533569)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f3a30",
   "metadata": {},
   "source": [
    "## Happy or Sad Classification CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02665c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                          ]       0 / 2670333\r",
      "  0% [                                                                          ]    8192 / 2670333\r",
      "  0% [                                                                          ]   16384 / 2670333\r",
      "  0% [                                                                          ]   24576 / 2670333\r",
      "  1% [                                                                          ]   32768 / 2670333\r",
      "  1% [.                                                                         ]   40960 / 2670333\r",
      "  1% [.                                                                         ]   49152 / 2670333\r",
      "  2% [.                                                                         ]   57344 / 2670333\r",
      "  2% [.                                                                         ]   65536 / 2670333\r",
      "  2% [..                                                                        ]   73728 / 2670333\r",
      "  3% [..                                                                        ]   81920 / 2670333\r",
      "  3% [..                                                                        ]   90112 / 2670333\r",
      "  3% [..                                                                        ]   98304 / 2670333\r",
      "  3% [..                                                                        ]  106496 / 2670333\r",
      "  4% [...                                                                       ]  114688 / 2670333\r",
      "  4% [...                                                                       ]  122880 / 2670333\r",
      "  4% [...                                                                       ]  131072 / 2670333\r",
      "  5% [...                                                                       ]  139264 / 2670333\r",
      "  5% [....                                                                      ]  147456 / 2670333\r",
      "  5% [....                                                                      ]  155648 / 2670333\r",
      "  6% [....                                                                      ]  163840 / 2670333\r",
      "  6% [....                                                                      ]  172032 / 2670333\r",
      "  6% [....                                                                      ]  180224 / 2670333\r",
      "  7% [.....                                                                     ]  188416 / 2670333\r",
      "  7% [.....                                                                     ]  196608 / 2670333\r",
      "  7% [.....                                                                     ]  204800 / 2670333\r",
      "  7% [.....                                                                     ]  212992 / 2670333\r",
      "  8% [......                                                                    ]  221184 / 2670333\r",
      "  8% [......                                                                    ]  229376 / 2670333\r",
      "  8% [......                                                                    ]  237568 / 2670333\r",
      "  9% [......                                                                    ]  245760 / 2670333\r",
      "  9% [.......                                                                   ]  253952 / 2670333\r",
      "  9% [.......                                                                   ]  262144 / 2670333\r",
      " 10% [.......                                                                   ]  270336 / 2670333\r",
      " 10% [.......                                                                   ]  278528 / 2670333\r",
      " 10% [.......                                                                   ]  286720 / 2670333\r",
      " 11% [........                                                                  ]  294912 / 2670333\r",
      " 11% [........                                                                  ]  303104 / 2670333\r",
      " 11% [........                                                                  ]  311296 / 2670333\r",
      " 11% [........                                                                  ]  319488 / 2670333\r",
      " 12% [.........                                                                 ]  327680 / 2670333\r",
      " 12% [.........                                                                 ]  335872 / 2670333\r",
      " 12% [.........                                                                 ]  344064 / 2670333\r",
      " 13% [.........                                                                 ]  352256 / 2670333\r",
      " 13% [.........                                                                 ]  360448 / 2670333\r",
      " 13% [..........                                                                ]  368640 / 2670333\r",
      " 14% [..........                                                                ]  376832 / 2670333\r",
      " 14% [..........                                                                ]  385024 / 2670333\r",
      " 14% [..........                                                                ]  393216 / 2670333\r",
      " 15% [...........                                                               ]  401408 / 2670333\r",
      " 15% [...........                                                               ]  409600 / 2670333\r",
      " 15% [...........                                                               ]  417792 / 2670333\r",
      " 15% [...........                                                               ]  425984 / 2670333\r",
      " 16% [............                                                              ]  434176 / 2670333\r",
      " 16% [............                                                              ]  442368 / 2670333\r",
      " 16% [............                                                              ]  450560 / 2670333\r",
      " 17% [............                                                              ]  458752 / 2670333\r",
      " 17% [............                                                              ]  466944 / 2670333\r",
      " 17% [.............                                                             ]  475136 / 2670333\r",
      " 18% [.............                                                             ]  483328 / 2670333\r",
      " 18% [.............                                                             ]  491520 / 2670333\r",
      " 18% [.............                                                             ]  499712 / 2670333\r",
      " 19% [..............                                                            ]  507904 / 2670333\r",
      " 19% [..............                                                            ]  516096 / 2670333\r",
      " 19% [..............                                                            ]  524288 / 2670333\r",
      " 19% [..............                                                            ]  532480 / 2670333\r",
      " 20% [..............                                                            ]  540672 / 2670333\r",
      " 20% [...............                                                           ]  548864 / 2670333\r",
      " 20% [...............                                                           ]  557056 / 2670333\r",
      " 21% [...............                                                           ]  565248 / 2670333\r",
      " 21% [...............                                                           ]  573440 / 2670333\r",
      " 21% [................                                                          ]  581632 / 2670333\r",
      " 22% [................                                                          ]  589824 / 2670333\r",
      " 22% [................                                                          ]  598016 / 2670333\r",
      " 22% [................                                                          ]  606208 / 2670333\r",
      " 23% [.................                                                         ]  614400 / 2670333\r",
      " 23% [.................                                                         ]  622592 / 2670333\r",
      " 23% [.................                                                         ]  630784 / 2670333\r",
      " 23% [.................                                                         ]  638976 / 2670333\r",
      " 24% [.................                                                         ]  647168 / 2670333\r",
      " 24% [..................                                                        ]  655360 / 2670333\r",
      " 24% [..................                                                        ]  663552 / 2670333\r",
      " 25% [..................                                                        ]  671744 / 2670333\r",
      " 25% [..................                                                        ]  679936 / 2670333\r",
      " 25% [...................                                                       ]  688128 / 2670333\r",
      " 26% [...................                                                       ]  696320 / 2670333\r",
      " 26% [...................                                                       ]  704512 / 2670333\r",
      " 26% [...................                                                       ]  712704 / 2670333\r",
      " 26% [...................                                                       ]  720896 / 2670333\r",
      " 27% [....................                                                      ]  729088 / 2670333\r",
      " 27% [....................                                                      ]  737280 / 2670333\r",
      " 27% [....................                                                      ]  745472 / 2670333\r",
      " 28% [....................                                                      ]  753664 / 2670333\r",
      " 28% [.....................                                                     ]  761856 / 2670333\r",
      " 28% [.....................                                                     ]  770048 / 2670333\r",
      " 29% [.....................                                                     ]  778240 / 2670333\r",
      " 29% [.....................                                                     ]  786432 / 2670333\r",
      " 29% [......................                                                    ]  794624 / 2670333\r",
      " 30% [......................                                                    ]  802816 / 2670333\r",
      " 30% [......................                                                    ]  811008 / 2670333\r",
      " 30% [......................                                                    ]  819200 / 2670333\r",
      " 30% [......................                                                    ]  827392 / 2670333\r",
      " 31% [.......................                                                   ]  835584 / 2670333\r",
      " 31% [.......................                                                   ]  843776 / 2670333\r",
      " 31% [.......................                                                   ]  851968 / 2670333\r",
      " 32% [.......................                                                   ]  860160 / 2670333\r",
      " 32% [........................                                                  ]  868352 / 2670333\r",
      " 32% [........................                                                  ]  876544 / 2670333\r",
      " 33% [........................                                                  ]  884736 / 2670333\r",
      " 33% [........................                                                  ]  892928 / 2670333\r",
      " 33% [........................                                                  ]  901120 / 2670333\r",
      " 34% [.........................                                                 ]  909312 / 2670333\r",
      " 34% [.........................                                                 ]  917504 / 2670333\r",
      " 34% [.........................                                                 ]  925696 / 2670333\r",
      " 34% [.........................                                                 ]  933888 / 2670333\r",
      " 35% [..........................                                                ]  942080 / 2670333\r",
      " 35% [..........................                                                ]  950272 / 2670333\r",
      " 35% [..........................                                                ]  958464 / 2670333\r",
      " 36% [..........................                                                ]  966656 / 2670333\r",
      " 36% [...........................                                               ]  974848 / 2670333\r",
      " 36% [...........................                                               ]  983040 / 2670333\r",
      " 37% [...........................                                               ]  991232 / 2670333\r",
      " 37% [...........................                                               ]  999424 / 2670333\r",
      " 37% [...........................                                               ] 1007616 / 2670333\r",
      " 38% [............................                                              ] 1015808 / 2670333\r",
      " 38% [............................                                              ] 1024000 / 2670333\r",
      " 38% [............................                                              ] 1032192 / 2670333\r",
      " 38% [............................                                              ] 1040384 / 2670333\r",
      " 39% [.............................                                             ] 1048576 / 2670333\r",
      " 39% [.............................                                             ] 1056768 / 2670333\r",
      " 39% [.............................                                             ] 1064960 / 2670333\r",
      " 40% [.............................                                             ] 1073152 / 2670333\r",
      " 40% [.............................                                             ] 1081344 / 2670333\r",
      " 40% [..............................                                            ] 1089536 / 2670333\r",
      " 41% [..............................                                            ] 1097728 / 2670333\r",
      " 41% [..............................                                            ] 1105920 / 2670333\r",
      " 41% [..............................                                            ] 1114112 / 2670333\r",
      " 42% [...............................                                           ] 1122304 / 2670333\r",
      " 42% [...............................                                           ] 1130496 / 2670333\r",
      " 42% [...............................                                           ] 1138688 / 2670333\r",
      " 42% [...............................                                           ] 1146880 / 2670333\r",
      " 43% [................................                                          ] 1155072 / 2670333\r",
      " 43% [................................                                          ] 1163264 / 2670333\r",
      " 43% [................................                                          ] 1171456 / 2670333\r",
      " 44% [................................                                          ] 1179648 / 2670333\r",
      " 44% [................................                                          ] 1187840 / 2670333\r",
      " 44% [.................................                                         ] 1196032 / 2670333\r",
      " 45% [.................................                                         ] 1204224 / 2670333\r",
      " 45% [.................................                                         ] 1212416 / 2670333\r",
      " 45% [.................................                                         ] 1220608 / 2670333\r",
      " 46% [..................................                                        ] 1228800 / 2670333\r",
      " 46% [..................................                                        ] 1236992 / 2670333\r",
      " 46% [..................................                                        ] 1245184 / 2670333\r",
      " 46% [..................................                                        ] 1253376 / 2670333\r",
      " 47% [..................................                                        ] 1261568 / 2670333\r",
      " 47% [...................................                                       ] 1269760 / 2670333\r",
      " 47% [...................................                                       ] 1277952 / 2670333\r",
      " 48% [...................................                                       ] 1286144 / 2670333\r",
      " 48% [...................................                                       ] 1294336 / 2670333\r",
      " 48% [....................................                                      ] 1302528 / 2670333\r",
      " 49% [....................................                                      ] 1310720 / 2670333\r",
      " 49% [....................................                                      ] 1318912 / 2670333\r",
      " 49% [....................................                                      ] 1327104 / 2670333\r",
      " 50% [.....................................                                     ] 1335296 / 2670333\r",
      " 50% [.....................................                                     ] 1343488 / 2670333\r",
      " 50% [.....................................                                     ] 1351680 / 2670333\r",
      " 50% [.....................................                                     ] 1359872 / 2670333\r",
      " 51% [.....................................                                     ] 1368064 / 2670333\r",
      " 51% [......................................                                    ] 1376256 / 2670333\r",
      " 51% [......................................                                    ] 1384448 / 2670333\r",
      " 52% [......................................                                    ] 1392640 / 2670333\r",
      " 52% [......................................                                    ] 1400832 / 2670333\r",
      " 52% [.......................................                                   ] 1409024 / 2670333\r",
      " 53% [.......................................                                   ] 1417216 / 2670333\r",
      " 53% [.......................................                                   ] 1425408 / 2670333\r",
      " 53% [.......................................                                   ] 1433600 / 2670333\r",
      " 53% [.......................................                                   ] 1441792 / 2670333\r",
      " 54% [........................................                                  ] 1449984 / 2670333\r",
      " 54% [........................................                                  ] 1458176 / 2670333\r",
      " 54% [........................................                                  ] 1466368 / 2670333\r",
      " 55% [........................................                                  ] 1474560 / 2670333\r",
      " 55% [.........................................                                 ] 1482752 / 2670333\r",
      " 55% [.........................................                                 ] 1490944 / 2670333\r",
      " 56% [.........................................                                 ] 1499136 / 2670333\r",
      " 56% [.........................................                                 ] 1507328 / 2670333\r",
      " 56% [.........................................                                 ] 1515520 / 2670333\r",
      " 57% [..........................................                                ] 1523712 / 2670333\r",
      " 57% [..........................................                                ] 1531904 / 2670333\r",
      " 57% [..........................................                                ] 1540096 / 2670333\r",
      " 57% [..........................................                                ] 1548288 / 2670333\r",
      " 58% [...........................................                               ] 1556480 / 2670333\r",
      " 58% [...........................................                               ] 1564672 / 2670333\r",
      " 58% [...........................................                               ] 1572864 / 2670333\r",
      " 59% [...........................................                               ] 1581056 / 2670333\r",
      " 59% [............................................                              ] 1589248 / 2670333\r",
      " 59% [............................................                              ] 1597440 / 2670333\r",
      " 60% [............................................                              ] 1605632 / 2670333\r",
      " 60% [............................................                              ] 1613824 / 2670333\r",
      " 60% [............................................                              ] 1622016 / 2670333\r",
      " 61% [.............................................                             ] 1630208 / 2670333\r",
      " 61% [.............................................                             ] 1638400 / 2670333\r",
      " 61% [.............................................                             ] 1646592 / 2670333\r",
      " 61% [.............................................                             ] 1654784 / 2670333\r",
      " 62% [..............................................                            ] 1662976 / 2670333\r",
      " 62% [..............................................                            ] 1671168 / 2670333\r",
      " 62% [..............................................                            ] 1679360 / 2670333\r",
      " 63% [..............................................                            ] 1687552 / 2670333\r",
      " 63% [..............................................                            ] 1695744 / 2670333\r",
      " 63% [...............................................                           ] 1703936 / 2670333\r",
      " 64% [...............................................                           ] 1712128 / 2670333\r",
      " 64% [...............................................                           ] 1720320 / 2670333\r",
      " 64% [...............................................                           ] 1728512 / 2670333\r",
      " 65% [................................................                          ] 1736704 / 2670333\r",
      " 65% [................................................                          ] 1744896 / 2670333\r",
      " 65% [................................................                          ] 1753088 / 2670333\r",
      " 65% [................................................                          ] 1761280 / 2670333\r",
      " 66% [.................................................                         ] 1769472 / 2670333\r",
      " 66% [.................................................                         ] 1777664 / 2670333\r",
      " 66% [.................................................                         ] 1785856 / 2670333\r",
      " 67% [.................................................                         ] 1794048 / 2670333\r",
      " 67% [.................................................                         ] 1802240 / 2670333\r",
      " 67% [..................................................                        ] 1810432 / 2670333\r",
      " 68% [..................................................                        ] 1818624 / 2670333\r",
      " 68% [..................................................                        ] 1826816 / 2670333\r",
      " 68% [..................................................                        ] 1835008 / 2670333\r",
      " 69% [...................................................                       ] 1843200 / 2670333\r",
      " 69% [...................................................                       ] 1851392 / 2670333\r",
      " 69% [...................................................                       ] 1859584 / 2670333\r",
      " 69% [...................................................                       ] 1867776 / 2670333\r",
      " 70% [...................................................                       ] 1875968 / 2670333\r",
      " 70% [....................................................                      ] 1884160 / 2670333\r",
      " 70% [....................................................                      ] 1892352 / 2670333\r",
      " 71% [....................................................                      ] 1900544 / 2670333\r",
      " 71% [....................................................                      ] 1908736 / 2670333\r",
      " 71% [.....................................................                     ] 1916928 / 2670333\r",
      " 72% [.....................................................                     ] 1925120 / 2670333\r",
      " 72% [.....................................................                     ] 1933312 / 2670333\r",
      " 72% [.....................................................                     ] 1941504 / 2670333\r",
      " 73% [......................................................                    ] 1949696 / 2670333\r",
      " 73% [......................................................                    ] 1957888 / 2670333\r",
      " 73% [......................................................                    ] 1966080 / 2670333\r",
      " 73% [......................................................                    ] 1974272 / 2670333\r",
      " 74% [......................................................                    ] 1982464 / 2670333\r",
      " 74% [.......................................................                   ] 1990656 / 2670333\r",
      " 74% [.......................................................                   ] 1998848 / 2670333\r",
      " 75% [.......................................................                   ] 2007040 / 2670333\r",
      " 75% [.......................................................                   ] 2015232 / 2670333\r",
      " 75% [........................................................                  ] 2023424 / 2670333\r",
      " 76% [........................................................                  ] 2031616 / 2670333\r",
      " 76% [........................................................                  ] 2039808 / 2670333\r",
      " 76% [........................................................                  ] 2048000 / 2670333\r",
      " 77% [........................................................                  ] 2056192 / 2670333\r",
      " 77% [.........................................................                 ] 2064384 / 2670333\r",
      " 77% [.........................................................                 ] 2072576 / 2670333\r",
      " 77% [.........................................................                 ] 2080768 / 2670333\r",
      " 78% [.........................................................                 ] 2088960 / 2670333\r",
      " 78% [..........................................................                ] 2097152 / 2670333\r",
      " 78% [..........................................................                ] 2105344 / 2670333\r",
      " 79% [..........................................................                ] 2113536 / 2670333\r",
      " 79% [..........................................................                ] 2121728 / 2670333\r",
      " 79% [...........................................................               ] 2129920 / 2670333\r",
      " 80% [...........................................................               ] 2138112 / 2670333\r",
      " 80% [...........................................................               ] 2146304 / 2670333\r",
      " 80% [...........................................................               ] 2154496 / 2670333\r",
      " 80% [...........................................................               ] 2162688 / 2670333\r",
      " 81% [............................................................              ] 2170880 / 2670333\r",
      " 81% [............................................................              ] 2179072 / 2670333\r",
      " 81% [............................................................              ] 2187264 / 2670333\r",
      " 82% [............................................................              ] 2195456 / 2670333\r",
      " 82% [.............................................................             ] 2203648 / 2670333\r",
      " 82% [.............................................................             ] 2211840 / 2670333\r",
      " 83% [.............................................................             ] 2220032 / 2670333\r",
      " 83% [.............................................................             ] 2228224 / 2670333\r",
      " 83% [.............................................................             ] 2236416 / 2670333\r",
      " 84% [..............................................................            ] 2244608 / 2670333\r",
      " 84% [..............................................................            ] 2252800 / 2670333\r",
      " 84% [..............................................................            ] 2260992 / 2670333\r",
      " 84% [..............................................................            ] 2269184 / 2670333\r",
      " 85% [...............................................................           ] 2277376 / 2670333\r",
      " 85% [...............................................................           ] 2285568 / 2670333\r",
      " 85% [...............................................................           ] 2293760 / 2670333\r",
      " 86% [...............................................................           ] 2301952 / 2670333\r",
      " 86% [................................................................          ] 2310144 / 2670333\r",
      " 86% [................................................................          ] 2318336 / 2670333\r",
      " 87% [................................................................          ] 2326528 / 2670333\r",
      " 87% [................................................................          ] 2334720 / 2670333\r",
      " 87% [................................................................          ] 2342912 / 2670333\r",
      " 88% [.................................................................         ] 2351104 / 2670333\r",
      " 88% [.................................................................         ] 2359296 / 2670333\r",
      " 88% [.................................................................         ] 2367488 / 2670333\r",
      " 88% [.................................................................         ] 2375680 / 2670333\r",
      " 89% [..................................................................        ] 2383872 / 2670333\r",
      " 89% [..................................................................        ] 2392064 / 2670333\r",
      " 89% [..................................................................        ] 2400256 / 2670333\r",
      " 90% [..................................................................        ] 2408448 / 2670333\r",
      " 90% [..................................................................        ] 2416640 / 2670333\r",
      " 90% [...................................................................       ] 2424832 / 2670333\r",
      " 91% [...................................................................       ] 2433024 / 2670333\r",
      " 91% [...................................................................       ] 2441216 / 2670333\r",
      " 91% [...................................................................       ] 2449408 / 2670333\r",
      " 92% [....................................................................      ] 2457600 / 2670333\r",
      " 92% [....................................................................      ] 2465792 / 2670333\r",
      " 92% [....................................................................      ] 2473984 / 2670333\r",
      " 92% [....................................................................      ] 2482176 / 2670333\r",
      " 93% [.....................................................................     ] 2490368 / 2670333\r",
      " 93% [.....................................................................     ] 2498560 / 2670333\r",
      " 93% [.....................................................................     ] 2506752 / 2670333\r",
      " 94% [.....................................................................     ] 2514944 / 2670333\r",
      " 94% [.....................................................................     ] 2523136 / 2670333\r",
      " 94% [......................................................................    ] 2531328 / 2670333\r",
      " 95% [......................................................................    ] 2539520 / 2670333\r",
      " 95% [......................................................................    ] 2547712 / 2670333\r",
      " 95% [......................................................................    ] 2555904 / 2670333\r",
      " 96% [.......................................................................   ] 2564096 / 2670333\r",
      " 96% [.......................................................................   ] 2572288 / 2670333\r",
      " 96% [.......................................................................   ] 2580480 / 2670333\r",
      " 96% [.......................................................................   ] 2588672 / 2670333\r",
      " 97% [.......................................................................   ] 2596864 / 2670333\r",
      " 97% [........................................................................  ] 2605056 / 2670333\r",
      " 97% [........................................................................  ] 2613248 / 2670333\r",
      " 98% [........................................................................  ] 2621440 / 2670333\r",
      " 98% [........................................................................  ] 2629632 / 2670333\r",
      " 98% [......................................................................... ] 2637824 / 2670333\r",
      " 99% [......................................................................... ] 2646016 / 2670333\r",
      " 99% [......................................................................... ] 2654208 / 2670333\r",
      " 99% [......................................................................... ] 2662400 / 2670333\r",
      "100% [..........................................................................] 2670333 / 2670333"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "DESIRED_ACCURACY = 0.999\n",
    "\n",
    "import wget\n",
    "wget.download(\"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/happy-or-sad.zip\")\n",
    "    \n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"happy-or-sad.zip\", 'r')\n",
    "zip_ref.extractall(\"/tmp/h-or-s\")\n",
    "zip_ref.close()\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>DESIRED_ACCURACY):\n",
    "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74147307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hos = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model_hos.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ea78120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        \"/tmp/h-or-s\",  \n",
    "        target_size=(150, 150), \n",
    "        batch_size=10,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Expected output: 'Found 80 images belonging to 2 classes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0da6bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 3.0103 - accuracy: 0.4535\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6495 - accuracy: 0.6958\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.6128 - accuracy: 0.7019\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3533 - accuracy: 0.9367\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3188 - accuracy: 0.8917\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1156 - accuracy: 0.9723\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2357 - accuracy: 0.8897\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2117 - accuracy: 0.8915\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0548 - accuracy: 0.9851\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0393 - accuracy: 0.9972\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0366 - accuracy: 0.9929\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "\n",
      "Reached 99.9% accuracy so cancelling training!\n"
     ]
    }
   ],
   "source": [
    "history = model_hos.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=15,\n",
    "      verbose=1,\n",
    "      callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56480777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
